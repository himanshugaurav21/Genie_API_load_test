{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Databricks Genie API Load Testing with MLflow Tracking\n",
        "\n",
        "This notebook demonstrates best practices for calling Databricks Genie Space API:\n",
        "- Exponential backoff with jitter\n",
        "- Retry logic for 429 rate limit errors\n",
        "- MLflow tracking for all API calls\n",
        "- Concurrent request handling\n",
        "- Configurable test scenarios\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import Dict, List, Optional, Any\n",
        "import requests\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_exponential,\n",
        "    retry_if_exception_type,\n",
        "    before_sleep_log\n",
        ")\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GENIE API CONNECTION CONFIGURATION\n",
        "# ============================================================\n",
        "WORKSPACE_URL = \"https://your-workspace.cloud.databricks.com\"  # Update this\n",
        "GENIE_SPACE_ID = \"your-genie-space-id\"  # Update this\n",
        "API_TOKEN = dbutils.secrets.get(scope=\"your-scope\", key=\"your-key\")  # Update this\n",
        "\n",
        "# ============================================================\n",
        "# MLFLOW EXPERIMENT CONFIGURATION\n",
        "# ============================================================\n",
        "MLFLOW_EXPERIMENT_NAME = \"/Shared/genie-load-test\"  # Change this to your desired experiment path\n",
        "# Examples:\n",
        "# - \"/Shared/genie-load-test\"\n",
        "# - \"/Users/your.email@company.com/genie-experiments\"\n",
        "# - \"/Teams/data-engineering/genie-api-tests\"\n",
        "\n",
        "# ============================================================\n",
        "# API RETRY CONFIGURATION\n",
        "# ============================================================\n",
        "MAX_RETRIES = 5           # Maximum number of retry attempts for 429 errors\n",
        "BASE_WAIT_TIME = 1        # Initial wait time in seconds (exponential backoff base)\n",
        "MAX_WAIT_TIME = 60        # Maximum wait time between retries in seconds\n",
        "JITTER_MAX = 2            # Maximum random jitter in seconds to prevent thundering herd\n",
        "TIMEOUT = 300             # Maximum time in seconds to wait for a single query response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Test Scenario Configuration\n",
        "\n",
        "**Configure your load test scenarios here by adjusting the parameters below.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LOAD TEST SCENARIO CONFIGURATIONS\n",
        "# ============================================================\n",
        "# Easily adjust these parameters to create different load test scenarios\n",
        "\n",
        "# Predefined Load Test Scenarios\n",
        "LOAD_TEST_SCENARIOS = {\n",
        "    \"single_test\": {\n",
        "        \"num_questions\": 1,\n",
        "        \"target_duration\": None,  # No spreading - immediate submission\n",
        "        \"max_workers\": 1,\n",
        "        \"description\": \"Single question test for validation\"\n",
        "    },\n",
        "    \"light_load\": {\n",
        "        \"num_questions\": 10,\n",
        "        \"target_duration\": 60,  # 10 questions over 60 seconds\n",
        "        \"max_workers\": 3,\n",
        "        \"description\": \"Light load - 10 questions over 1 minute\"\n",
        "    },\n",
        "    \"moderate_load\": {\n",
        "        \"num_questions\": 20,\n",
        "        \"target_duration\": 30,  # 20 questions over 30 seconds\n",
        "        \"max_workers\": 5,\n",
        "        \"description\": \"Moderate load - 20 questions over 30 seconds\"\n",
        "    },\n",
        "    \"high_load\": {\n",
        "        \"num_questions\": 35,\n",
        "        \"target_duration\": 30,  # 35 questions over 30 seconds\n",
        "        \"max_workers\": 10,\n",
        "        \"description\": \"High load - 35 questions over 30 seconds\"\n",
        "    },\n",
        "    \"stress_test\": {\n",
        "        \"num_questions\": 50,\n",
        "        \"target_duration\": 20,  # 50 questions over 20 seconds\n",
        "        \"max_workers\": 15,\n",
        "        \"description\": \"Stress test - 50 questions over 20 seconds\"\n",
        "    },\n",
        "    \"burst_test\": {\n",
        "        \"num_questions\": 50,\n",
        "        \"target_duration\": None,  # No spreading - all at once\n",
        "        \"max_workers\": 20,\n",
        "        \"description\": \"Burst test - 50 questions submitted immediately\"\n",
        "    },\n",
        "    \"sustained_load\": {\n",
        "        \"num_questions\": 100,\n",
        "        \"target_duration\": 120,  # 100 questions over 2 minutes\n",
        "        \"max_workers\": 10,\n",
        "        \"description\": \"Sustained load - 100 questions over 2 minutes\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# ACTIVE SCENARIO SELECTION\n",
        "# ============================================================\n",
        "# Select which scenario to run (change this to switch scenarios)\n",
        "ACTIVE_SCENARIO = \"high_load\"  # Options: single_test, light_load, moderate_load, high_load, stress_test, burst_test, sustained_load\n",
        "\n",
        "# OR - Create a custom scenario with your own parameters\n",
        "# Uncomment and modify the lines below to use custom parameters instead\n",
        "\n",
        "CUSTOM_SCENARIO = {\n",
        "    \"num_questions\": 35,        # Total number of questions to submit\n",
        "    \"target_duration\": 30,      # Duration in seconds to spread questions over (None = submit all immediately)\n",
        "    \"max_workers\": 10,          # Maximum concurrent threads/workers\n",
        "    \"description\": \"Custom load test scenario\"\n",
        "}\n",
        "\n",
        "# Set to True to use CUSTOM_SCENARIO instead of ACTIVE_SCENARIO\n",
        "USE_CUSTOM_SCENARIO = False\n",
        "\n",
        "# ============================================================\n",
        "# Helper function to get active scenario configuration\n",
        "# ============================================================\n",
        "def get_active_scenario():\n",
        "    \"\"\"Get the currently active scenario configuration\"\"\"\n",
        "    if USE_CUSTOM_SCENARIO:\n",
        "        scenario = CUSTOM_SCENARIO\n",
        "        scenario_name = \"custom\"\n",
        "    else:\n",
        "        scenario = LOAD_TEST_SCENARIOS.get(ACTIVE_SCENARIO)\n",
        "        scenario_name = ACTIVE_SCENARIO\n",
        "        if scenario is None:\n",
        "            raise ValueError(f\"Unknown scenario: {ACTIVE_SCENARIO}. Available: {list(LOAD_TEST_SCENARIOS.keys())}\")\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ACTIVE LOAD TEST SCENARIO: {scenario_name.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Description: {scenario['description']}\")\n",
        "    print(f\"Number of Questions: {scenario['num_questions']}\")\n",
        "    print(f\"Target Duration: {scenario['target_duration']} seconds\" if scenario['target_duration'] else \"Target Duration: None (immediate submission)\")\n",
        "    print(f\"Max Workers: {scenario['max_workers']}\")\n",
        "    if scenario['target_duration']:\n",
        "        rate = scenario['num_questions'] / scenario['target_duration']\n",
        "        print(f\"Expected Rate: {rate:.2f} questions/second\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    return scenario_name, scenario\n",
        "\n",
        "# Display active scenario\n",
        "scenario_name, scenario_config = get_active_scenario()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Rate Limit Exception Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RateLimitError(Exception):\n",
        "    \"\"\"Custom exception for rate limit errors (429)\"\"\"\n",
        "    pass\n",
        "\n",
        "class GenieAPIError(Exception):\n",
        "    \"\"\"Custom exception for Genie API errors\"\"\"\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GenieAPIClient:\n",
        "    \"\"\"\n",
        "    Client for interacting with Databricks Genie Space API.\n",
        "    Implements exponential backoff, jitter, and retry logic.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        workspace_url: str,\n",
        "        space_id: str,\n",
        "        token: str,\n",
        "        max_retries: int = MAX_RETRIES,\n",
        "        base_wait: int = BASE_WAIT_TIME,\n",
        "        max_wait: int = MAX_WAIT_TIME\n",
        "    ):\n",
        "        self.workspace_url = workspace_url.rstrip('/')\n",
        "        self.space_id = space_id\n",
        "        self.token = token\n",
        "        self.max_retries = max_retries\n",
        "        self.base_wait = base_wait\n",
        "        self.max_wait = max_wait\n",
        "        self.base_url = f\"{self.workspace_url}/api/2.0/genie/spaces/{self.space_id}\"\n",
        "        \n",
        "    def _get_headers(self) -> Dict[str, str]:\n",
        "        \"\"\"Get request headers with authentication\"\"\"\n",
        "        return {\n",
        "            \"Authorization\": f\"Bearer {self.token}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "    \n",
        "    def _add_jitter(self, wait_time: float) -> float:\n",
        "        \"\"\"Add random jitter to wait time to prevent thundering herd\"\"\"\n",
        "        jitter = random.uniform(0, JITTER_MAX)\n",
        "        return wait_time + jitter\n",
        "    \n",
        "    @retry(\n",
        "        retry=retry_if_exception_type(RateLimitError),\n",
        "        stop=stop_after_attempt(MAX_RETRIES),\n",
        "        wait=wait_exponential(multiplier=BASE_WAIT_TIME, max=MAX_WAIT_TIME),\n",
        "        before_sleep=before_sleep_log(logger, logging.WARNING)\n",
        "    )\n",
        "    def _make_request(\n",
        "        self,\n",
        "        method: str,\n",
        "        endpoint: str,\n",
        "        data: Optional[Dict] = None,\n",
        "        params: Optional[Dict] = None\n",
        "    ) -> Dict:\n",
        "        \"\"\"Make HTTP request with retry logic for rate limiting\"\"\"\n",
        "        url = f\"{self.base_url}/{endpoint}\"\n",
        "        \n",
        "        try:\n",
        "            response = requests.request(\n",
        "                method=method,\n",
        "                url=url,\n",
        "                headers=self._get_headers(),\n",
        "                json=data,\n",
        "                params=params,\n",
        "                timeout=30\n",
        "            )\n",
        "            \n",
        "            # Handle rate limiting\n",
        "            if response.status_code == 429:\n",
        "                retry_after = int(response.headers.get('Retry-After', self.base_wait))\n",
        "                wait_time = self._add_jitter(retry_after)\n",
        "                logger.warning(f\"Rate limited. Waiting {wait_time:.2f} seconds before retry\")\n",
        "                time.sleep(wait_time)\n",
        "                raise RateLimitError(\"Rate limit exceeded (429)\")\n",
        "            \n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "            \n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            logger.error(f\"HTTP Error: {e}\")\n",
        "            raise GenieAPIError(f\"API request failed: {e}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Request Error: {e}\")\n",
        "            raise GenieAPIError(f\"Request failed: {e}\")\n",
        "    \n",
        "    def start_conversation(self, content: str) -> Dict:\n",
        "        \"\"\"Start a new conversation with a question\"\"\"\n",
        "        data = {\"content\": content}\n",
        "        return self._make_request(\"POST\", \"start-conversation\", data=data)\n",
        "    \n",
        "    def get_message_query_result(self, conversation_id: str, message_id: str) -> Dict:\n",
        "        \"\"\"Get the query result for a message\"\"\"\n",
        "        endpoint = f\"conversations/{conversation_id}/messages/{message_id}/query-result\"\n",
        "        return self._make_request(\"GET\", endpoint)\n",
        "    \n",
        "    def wait_for_result(\n",
        "        self,\n",
        "        conversation_id: str,\n",
        "        message_id: str,\n",
        "        timeout: int = TIMEOUT,\n",
        "        poll_interval: int = 2\n",
        "    ) -> Dict:\n",
        "        \"\"\"Poll for query result until completion or timeout\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        while time.time() - start_time < timeout:\n",
        "            try:\n",
        "                result = self.get_message_query_result(conversation_id, message_id)\n",
        "                status = result.get(\"status\")\n",
        "                \n",
        "                if status == \"COMPLETED\":\n",
        "                    return result\n",
        "                elif status == \"FAILED\":\n",
        "                    raise GenieAPIError(f\"Query failed: {result.get('error', 'Unknown error')}\")\n",
        "                elif status in [\"EXECUTING_QUERY\", \"QUERYING_HISTORY\", \"SUBMITTED\"]:\n",
        "                    time.sleep(poll_interval)\n",
        "                else:\n",
        "                    logger.warning(f\"Unknown status: {status}\")\n",
        "                    time.sleep(poll_interval)\n",
        "                    \n",
        "            except RateLimitError:\n",
        "                # The retry decorator will handle this\n",
        "                raise\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error polling for result: {e}\")\n",
        "                raise\n",
        "        \n",
        "        raise TimeoutError(f\"Query timed out after {timeout} seconds\")\n",
        "    \n",
        "    def ask_question(self, question: str, timeout: int = TIMEOUT) -> Dict:\n",
        "        \"\"\"Ask a question and wait for the result\"\"\"\n",
        "        # Start conversation\n",
        "        start_response = self.start_conversation(question)\n",
        "        conversation_id = start_response[\"conversation_id\"]\n",
        "        message_id = start_response[\"message_id\"]\n",
        "        \n",
        "        # Wait for result\n",
        "        result = self.wait_for_result(conversation_id, message_id, timeout)\n",
        "        \n",
        "        return {\n",
        "            \"conversation_id\": conversation_id,\n",
        "            \"message_id\": message_id,\n",
        "            \"question\": question,\n",
        "            \"result\": result\n",
        "        }\n",
        "\n",
        "def log_question_to_mlflow(\n",
        "    question: str,\n",
        "    response: Dict,\n",
        "    duration: float,\n",
        "    error: Optional[str] = None,\n",
        "    run_name: Optional[str] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Log a single question and its response to MLflow\n",
        "    Returns the run_id\n",
        "    \"\"\"\n",
        "    with mlflow.start_run(run_name=run_name, nested=True) as run:\n",
        "        # Log parameters\n",
        "        mlflow.log_param(\"question\", question)\n",
        "        mlflow.log_param(\"timestamp\", datetime.now().isoformat())\n",
        "        \n",
        "        # Log metrics\n",
        "        mlflow.log_metric(\"duration_seconds\", duration)\n",
        "        mlflow.log_metric(\"success\", 1 if error is None else 0)\n",
        "        \n",
        "        if error:\n",
        "            mlflow.log_param(\"error\", error)\n",
        "            mlflow.set_tag(\"status\", \"failed\")\n",
        "        else:\n",
        "            mlflow.set_tag(\"status\", \"success\")\n",
        "            \n",
        "            # Log conversation details\n",
        "            mlflow.log_param(\"conversation_id\", response.get(\"conversation_id\"))\n",
        "            mlflow.log_param(\"message_id\", response.get(\"message_id\"))\n",
        "            \n",
        "            # Log result details if available\n",
        "            result = response.get(\"result\", {})\n",
        "            if \"statement_response\" in result:\n",
        "                statement_response = result[\"statement_response\"]\n",
        "                if \"result_data\" in statement_response:\n",
        "                    result_data = statement_response[\"result_data\"]\n",
        "                    mlflow.log_metric(\"row_count\", result_data.get(\"row_count\", 0))\n",
        "            \n",
        "            # Save full response as artifact\n",
        "            with open(\"response.json\", \"w\") as f:\n",
        "                json.dump(response, f, indent=2)\n",
        "            mlflow.log_artifact(\"response.json\")\n",
        "        \n",
        "        return run.info.run_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Test Orchestrator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LoadTestOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates load testing with concurrent requests and MLflow tracking\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, client: GenieAPIClient, experiment_name: str = \"/Shared/genie-load-test\"):\n",
        "        self.client = client\n",
        "        self.experiment_name = experiment_name\n",
        "        mlflow.set_experiment(self.experiment_name)\n",
        "    \n",
        "    def _execute_single_question(\n",
        "        self,\n",
        "        question: str,\n",
        "        question_index: int,\n",
        "        delay: float = 0\n",
        "    ) -> Dict:\n",
        "        \"\"\"Execute a single question with optional delay\"\"\"\n",
        "        if delay > 0:\n",
        "            time.sleep(delay)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        error = None\n",
        "        response = None\n",
        "        \n",
        "        try:\n",
        "            logger.info(f\"[Q{question_index}] Asking: {question[:50]}...\")\n",
        "            response = self.client.ask_question(question)\n",
        "            duration = time.time() - start_time\n",
        "            logger.info(f\"[Q{question_index}] Completed in {duration:.2f}s\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            duration = time.time() - start_time\n",
        "            error = str(e)\n",
        "            logger.error(f\"[Q{question_index}] Failed after {duration:.2f}s: {error}\")\n",
        "        \n",
        "        # Log to MLflow\n",
        "        run_name = f\"question_{question_index}\"\n",
        "        run_id = log_question_to_mlflow(question, response or {}, duration, error, run_name)\n",
        "        \n",
        "        return {\n",
        "            \"question_index\": question_index,\n",
        "            \"question\": question,\n",
        "            \"duration\": duration,\n",
        "            \"success\": error is None,\n",
        "            \"error\": error,\n",
        "            \"response\": response,\n",
        "            \"run_id\": run_id\n",
        "        }\n",
        "    \n",
        "    def run_load_test(\n",
        "        self,\n",
        "        questions: List[str],\n",
        "        max_workers: int = 10,\n",
        "        target_duration: Optional[float] = None,\n",
        "        test_name: str = \"load_test\"\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Run load test with multiple questions\n",
        "        \n",
        "        Args:\n",
        "            questions: List of questions to ask\n",
        "            max_workers: Maximum number of concurrent threads\n",
        "            target_duration: Target duration in seconds to spread questions over (e.g., 30 for \"35 questions in 30 seconds\")\n",
        "            test_name: Name for the MLflow run\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame with test results\n",
        "        \"\"\"\n",
        "        with mlflow.start_run(run_name=test_name) as parent_run:\n",
        "            # Log test configuration\n",
        "            mlflow.log_param(\"num_questions\", len(questions))\n",
        "            mlflow.log_param(\"max_workers\", max_workers)\n",
        "            mlflow.log_param(\"target_duration\", target_duration or \"None\")\n",
        "            mlflow.log_param(\"test_name\", test_name)\n",
        "            mlflow.log_param(\"start_time\", datetime.now().isoformat())\n",
        "            \n",
        "            # Calculate delays if target duration is specified\n",
        "            delays = [0] * len(questions)\n",
        "            if target_duration and len(questions) > 1:\n",
        "                interval = target_duration / len(questions)\n",
        "                delays = [i * interval for i in range(len(questions))]\n",
        "            \n",
        "            # Execute questions concurrently\n",
        "            results = []\n",
        "            test_start_time = time.time()\n",
        "            \n",
        "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "                futures = {\n",
        "                    executor.submit(\n",
        "                        self._execute_single_question,\n",
        "                        question,\n",
        "                        i,\n",
        "                        delays[i]\n",
        "                    ): i\n",
        "                    for i, question in enumerate(questions)\n",
        "                }\n",
        "                \n",
        "                for future in as_completed(futures):\n",
        "                    try:\n",
        "                        result = future.result()\n",
        "                        results.append(result)\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Unexpected error: {e}\")\n",
        "            \n",
        "            total_duration = time.time() - test_start_time\n",
        "            \n",
        "            # Log summary metrics\n",
        "            successful = sum(1 for r in results if r[\"success\"])\n",
        "            failed = len(results) - successful\n",
        "            avg_duration = sum(r[\"duration\"] for r in results) / len(results) if results else 0\n",
        "            \n",
        "            mlflow.log_metric(\"total_duration_seconds\", total_duration)\n",
        "            mlflow.log_metric(\"successful_questions\", successful)\n",
        "            mlflow.log_metric(\"failed_questions\", failed)\n",
        "            mlflow.log_metric(\"success_rate\", successful / len(results) if results else 0)\n",
        "            mlflow.log_metric(\"avg_question_duration\", avg_duration)\n",
        "            mlflow.log_metric(\"throughput_qps\", len(results) / total_duration if total_duration > 0 else 0)\n",
        "            \n",
        "            # Create results DataFrame\n",
        "            df = pd.DataFrame(results)\n",
        "            \n",
        "            # Save results\n",
        "            results_file = \"load_test_results.csv\"\n",
        "            df.to_csv(results_file, index=False)\n",
        "            mlflow.log_artifact(results_file)\n",
        "            \n",
        "            logger.info(f\"\\n{'='*60}\")\n",
        "            logger.info(f\"Load Test Summary: {test_name}\")\n",
        "            logger.info(f\"{'='*60}\")\n",
        "            logger.info(f\"Total Questions: {len(results)}\")\n",
        "            logger.info(f\"Successful: {successful}\")\n",
        "            logger.info(f\"Failed: {failed}\")\n",
        "            logger.info(f\"Success Rate: {successful / len(results) * 100:.2f}%\")\n",
        "            logger.info(f\"Total Duration: {total_duration:.2f}s\")\n",
        "            logger.info(f\"Avg Question Duration: {avg_duration:.2f}s\")\n",
        "            logger.info(f\"Throughput: {len(results) / total_duration:.2f} QPS\")\n",
        "            logger.info(f\"{'='*60}\\n\")\n",
        "            \n",
        "            return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample questions for testing\n",
        "SAMPLE_QUESTIONS = [\n",
        "    \"What is the total sales for last month?\",\n",
        "    \"Show me top 10 customers by revenue\",\n",
        "    \"What are the sales trends over the last 6 months?\",\n",
        "    \"Which products have the highest profit margin?\",\n",
        "    \"What is the average order value?\",\n",
        "    \"Show me sales by region\",\n",
        "    \"What is the customer churn rate?\",\n",
        "    \"How many new customers joined last month?\",\n",
        "    \"What is the inventory level for top products?\",\n",
        "    \"Show me the sales forecast for next quarter\"\n",
        "]\n",
        "\n",
        "# Generate more questions for load testing\n",
        "def generate_test_questions(base_questions: List[str], count: int) -> List[str]:\n",
        "    \"\"\"Generate test questions by repeating and varying base questions\"\"\"\n",
        "    questions = []\n",
        "    for i in range(count):\n",
        "        base_question = base_questions[i % len(base_questions)]\n",
        "        questions.append(f\"{base_question} (iteration {i // len(base_questions) + 1})\")\n",
        "    return questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Initialize Client and Orchestrator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Genie API client\n",
        "client = GenieAPIClient(\n",
        "    workspace_url=WORKSPACE_URL,\n",
        "    space_id=GENIE_SPACE_ID,\n",
        "    token=API_TOKEN\n",
        ")\n",
        "\n",
        "# Initialize the load test orchestrator with configured MLflow experiment\n",
        "orchestrator = LoadTestOrchestrator(\n",
        "    client=client,\n",
        "    experiment_name=MLFLOW_EXPERIMENT_NAME\n",
        ")\n",
        "\n",
        "print(\"✓ Client and orchestrator initialized successfully\")\n",
        "print(f\"✓ MLflow Experiment: {MLFLOW_EXPERIMENT_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Run Load Test with Active Scenario\n",
        "\n",
        "**This cell runs the load test using the active scenario configured in Section 2.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Test with Active Scenario\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the active scenario configuration from Section 2\n",
        "scenario_name, config = get_active_scenario()\n",
        "\n",
        "# Generate test questions based on configured number\n",
        "test_questions = generate_test_questions(SAMPLE_QUESTIONS, config['num_questions'])\n",
        "\n",
        "# Run the load test with the configured parameters\n",
        "results_df = orchestrator.run_load_test(\n",
        "    questions=test_questions,\n",
        "    max_workers=config['max_workers'],\n",
        "    target_duration=config['target_duration'],\n",
        "    test_name=f\"load_test_{scenario_name}\"\n",
        ")\n",
        "\n",
        "# Display results summary\n",
        "display(results_df[['question_index', 'question', 'duration', 'success', 'error']].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Analyze Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_results(df: pd.DataFrame):\n",
        "    \"\"\"Analyze and visualize test results\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DETAILED ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Success rate\n",
        "    success_rate = df['success'].mean() * 100\n",
        "    print(f\"\\nSuccess Rate: {success_rate:.2f}%\")\n",
        "    \n",
        "    # Duration statistics\n",
        "    print(\"\\nDuration Statistics (seconds):\")\n",
        "    print(f\"  Min: {df['duration'].min():.2f}\")\n",
        "    print(f\"  Max: {df['duration'].max():.2f}\")\n",
        "    print(f\"  Mean: {df['duration'].mean():.2f}\")\n",
        "    print(f\"  Median: {df['duration'].median():.2f}\")\n",
        "    print(f\"  Std Dev: {df['duration'].std():.2f}\")\n",
        "    \n",
        "    # Percentiles\n",
        "    print(\"\\nDuration Percentiles (seconds):\")\n",
        "    print(f\"  P50: {df['duration'].quantile(0.50):.2f}\")\n",
        "    print(f\"  P75: {df['duration'].quantile(0.75):.2f}\")\n",
        "    print(f\"  P90: {df['duration'].quantile(0.90):.2f}\")\n",
        "    print(f\"  P95: {df['duration'].quantile(0.95):.2f}\")\n",
        "    print(f\"  P99: {df['duration'].quantile(0.99):.2f}\")\n",
        "    \n",
        "    # Error analysis\n",
        "    if not df['success'].all():\n",
        "        print(\"\\nError Summary:\")\n",
        "        error_counts = df[df['error'].notna()]['error'].value_counts()\n",
        "        for error, count in error_counts.items():\n",
        "            print(f\"  {error}: {count}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Analyze the test results\n",
        "analyzed_df = analyze_results(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Run Additional Custom Test (Optional)\n",
        "\n",
        "**You can run an additional test with different parameters without changing the main configuration.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Run an additional test with custom parameters\n",
        "# Modify these values as needed\n",
        "CUSTOM_NUM_QUESTIONS = 20\n",
        "CUSTOM_TARGET_DURATION = 60  # seconds (None = immediate submission)\n",
        "CUSTOM_MAX_WORKERS = 5\n",
        "\n",
        "# Generate and run custom test\n",
        "custom_questions = generate_test_questions(SAMPLE_QUESTIONS, CUSTOM_NUM_QUESTIONS)\n",
        "custom_results_df = orchestrator.run_load_test(\n",
        "    questions=custom_questions,\n",
        "    max_workers=CUSTOM_MAX_WORKERS,\n",
        "    target_duration=CUSTOM_TARGET_DURATION,\n",
        "    test_name=\"custom_adhoc_test\"\n",
        ")\n",
        "\n",
        "# Analyze custom results\n",
        "analyze_results(custom_results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. View MLflow Tracking UI\n",
        "\n",
        "To view the detailed logs and metrics in MLflow:\n",
        "\n",
        "1. Navigate to your Databricks workspace\n",
        "2. Go to the MLflow Experiments page\n",
        "3. Find the experiment with the name configured in `MLFLOW_EXPERIMENT_NAME` (Section 1)\n",
        "   - Default: `/Shared/genie-load-test`\n",
        "   - You can change this in Section 1 to organize your experiments\n",
        "4. Click on any run to see detailed metrics, parameters, and artifacts\n",
        "\n",
        "Each question execution is logged as a nested run with:\n",
        "- Question text\n",
        "- Duration\n",
        "- Success/failure status\n",
        "- Full API response (as artifact)\n",
        "- Conversation and message IDs\n",
        "- Row counts (if applicable)\n",
        "\n",
        "**Tip**: Use different experiment names for different testing purposes:\n",
        "- `/Shared/genie-load-test-prod` - Production load tests\n",
        "- `/Shared/genie-load-test-dev` - Development tests\n",
        "- `/Users/your.email@company.com/genie-experiments` - Personal experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
